<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Visualizing Parallelism in Transformer Compute</title>
    <meta
      name="description"
      content="A step-by-step, diagram-driven explanation of distributed compute in transformer training: DP, TP, SP, CP, EP, VP."
    />
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <header class="hero">
      <div class="container">
        <p class="eyebrow">Distributed Training Notes</p>
        <h1>Visualizing Parallelism in Transformer Compute</h1>
        <p class="subtitle">
          This page is a map, not a manual. It keeps the tensor shapes visible,
          names the collectives at each step, and shows how each GPU’s local
          work is stitched into the global result. The emphasis is on clarity:
          where the sequence is gathered, where weights are sharded, and how the
          model returns to its memory‑efficient local form after each block.
        </p>
        <div class="hero-meta">
          <span>Author: Harshitha M</span>
          <span>Updated: Feb 2, 2026</span>
        </div>
      </div>
    </header>

    <nav class="top-nav">
      <div class="container">
        <a href="#reading-guide">Reading guide</a>
        <a href="#symbols">Symbols</a>
        <a href="#embeddings">Embeddings</a>
        <a href="#attention">Attention</a>
        <a href="#mlp">MLP</a>
        <a href="#moe">MoE</a>
        <a href="#loss">Loss</a>
      </div>
    </nav>

    <main class="container layout">
      <aside class="toc">
        <div class="toc-card">
          <h2>Contents</h2>
          <a href="#reading-guide">How to read</a>
          <a href="#symbols">Symbols</a>
          <a href="#local-shape">Local shape</a>
          <a href="#overview">Overview</a>
          <a href="#embeddings">Embeddings</a>
          <a href="#attention">Attention</a>
          <a href="#mlp">MLP</a>
          <a href="#moe">MoE</a>
          <a href="#loss">Loss</a>
          <a href="#fsdp">FSDP note</a>
        </div>
      </aside>

      <div class="content">
      <section class="card" id="reading-guide">
        <h2>How to Read the Diagrams</h2>
        <p>
          Think from the point of view of a <strong>single GPU</strong>. Each
          diagram answers two questions: (1) what local tensor shape does this
          GPU hold, and (2) which collective operations are needed to connect
          local compute into a correct global result.
        </p>
        <p>
          When you see the word <em>local</em>, it means the tensor fragment on
          one device. When you see a collective (AllGather, ReduceScatter,
          AllReduce, AllToAll), it means the GPU is coordinating with its
          process group to assemble or redistribute a global tensor.
        </p>
        <div class="reading-list">
          <div>
            <h3>Reading cues</h3>
            <ul>
              <li>Rectangles: local compute blocks.</li>
              <li>Arrows: data movement between blocks.</li>
              <li>Collectives: synchronization boundaries.</li>
            </ul>
          </div>
          <div>
            <h3>Goal</h3>
            <ul>
              <li>Track how a local shard becomes global.</li>
              <li>Track how global work returns to a shard.</li>
            </ul>
          </div>
        </div>
      </section>

      <section class="grid-two" id="symbols">
        <div class="card">
          <h2>Model Shape Symbols</h2>
          <table>
            <thead>
              <tr>
                <th>Symbol</th>
                <th>Meaning</th>
                <th>Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>B</td>
                <td>Batch size</td>
                <td>Global batch count (number of sequences)</td>
              </tr>
              <tr>
                <td>S</td>
                <td>Sequence length</td>
                <td>Context window size</td>
              </tr>
              <tr>
                <td>D</td>
                <td>Hidden size</td>
                <td>Width of the residual stream</td>
              </tr>
              <tr>
                <td>V</td>
                <td>Vocabulary size</td>
                <td>Total token count in the embedding table</td>
              </tr>
              <tr>
                <td>F</td>
                <td>FFN size</td>
                <td>Expansion dimension in the MLP</td>
              </tr>
              <tr>
                <td>E</td>
                <td>Number of experts</td>
                <td>Total experts in an MoE layer</td>
              </tr>
              <tr>
                <td>C</td>
                <td>Capacity</td>
                <td>Max tokens per expert (MoE specific)</td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="card">
          <h2>Parallel Configuration</h2>
          <table>
            <thead>
              <tr>
                <th>Symbol</th>
                <th>Parallelism</th>
                <th>Shard Target</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>dp</td>
                <td>Data parallel</td>
                <td>Batch dimension (B)</td>
              </tr>
              <tr>
                <td>tp</td>
                <td>Tensor parallel</td>
                <td>Weights (heads in attention, F in MLP)</td>
              </tr>
              <tr>
                <td>sp</td>
                <td>Sequence parallel</td>
                <td>Sequence activations (S)</td>
              </tr>
              <tr>
                <td>cp</td>
                <td>Context parallel</td>
                <td>Sequence dimension in attention (QKV)</td>
              </tr>
              <tr>
                <td>ep</td>
                <td>Expert parallel</td>
                <td>Experts (E) in MoE layers</td>
              </tr>
              <tr>
                <td>vp</td>
                <td>Vocab parallel</td>
                <td>Vocabulary table (V)</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>

      <section class="card highlight" id="local-shape">
        <h2>The Local Shape You Will See Repeatedly</h2>
        <p>
          A typical block enters and exits with the same local activation shape:
        </p>
        <p class="code-block">
          <code>[B/dp, S/(cp*sp), D]</code>
        </p>
        <p>
          Read this as: this GPU holds a fraction of the batch, a small slice of
          the sequence (sharded by context and sequence parallelism), and the
          full hidden dimension.
        </p>
      </section>

      <section class="card" id="overview">
        <h2>Overview of the Transformer Layer</h2>
        <p>
          A standard layer consists of embeddings (global input lookup), an
          attention block, and an MLP (dense or MoE). Each block starts from a
          local sequence shard, temporarily gathers what it needs, computes, and
          then returns to the memory-efficient local shape via collective
          operations.
        </p>
        <p class="section-lead">
          The rest of the page walks through the same rhythm repeatedly:
          gather what you need, compute locally, then reduce or scatter back to
          the local shape.
        </p>
        <div class="reading-list">
          <div>
            <h3>Assumed mesh</h3>
            <ul>
              <li>dp shards batch (B)</li>
              <li>cp and sp shard sequence (S)</li>
              <li>tp shards weights (heads or FFN dim)</li>
              <li>vp shards vocabulary (V)</li>
            </ul>
          </div>
          <div>
            <h3>Local entry/exit</h3>
            <ul>
              <li>[B/dp, S/(cp*sp), D]</li>
              <li>Full hidden dim D stays local</li>
            </ul>
          </div>
        </div>
      </section>

      <section class="card" id="embeddings">
        <h2>Embeddings (Vocab Parallel)</h2>
        <div class="diagram">
          <svg viewBox="0 0 500 500" role="img" aria-label="Embedding flow">
            <defs>
              <marker id="arrow" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                <path d="M0,0 L10,3 L0,6 Z" class="arrow-head" />
              </marker>
            </defs>
            
            <!-- Step 1: Input -->
            <rect x="150" y="20" width="200" height="50" class="box-meta" />
            <text x="250" y="45" class="text-label">Local Input Token</text>

            <line x1="250" y1="70" x2="250" y2="110" class="arrow-line" marker-end="url(#arrow)" />
            <text x="320" y="90" class="text-shape">B/dp, S/cp</text>

            <!-- Step 2: Mask -->
            <rect x="100" y="110" width="300" height="60" class="box-compute" />
            <text x="250" y="130" class="text-label">Mask</text>
            <text x="250" y="150" class="text-sub">(Zero out indices outside local vocab)</text>

            <line x1="250" y1="170" x2="250" y2="210" class="arrow-line" marker-end="url(#arrow)" />
            <text x="320" y="190" class="text-shape">B/dp, S/cp</text>

            <!-- Step 3: Lookup -->
            <rect x="100" y="210" width="300" height="60" class="box-compute" />
            <text x="250" y="230" class="text-label">Local Embedding Lookup</text>
            <text x="250" y="250" class="text-sub">weight: V/vp, D</text>

            <line x1="250" y1="270" x2="250" y2="310" class="arrow-line" marker-end="url(#arrow)" />
            <text x="350" y="290" class="text-shape">B/dp, S/cp, D (unreduced)</text>

            <!-- Step 4: ReduceScatter -->
            <rect x="100" y="310" width="300" height="80" class="box-collective" />
            <text x="250" y="335" class="text-label">ReduceScatter</text>
            <text x="250" y="355" class="text-sub">sum, reduce_over='vp'</text>
            <text x="250" y="375" class="text-sub">scatter_over='sp'</text>

            <line x1="250" y1="390" x2="250" y2="430" class="arrow-line" marker-end="url(#arrow)" />
            <text x="320" y="410" class="text-shape">B/dp, S/(cp*sp), D</text>
            
             <!-- Step 5: Output -->
            <rect x="248" y="430" width="4" height="4" fill="#000" />
          </svg>
        </div>
        <p class="diagram-caption">
          Diagram 1: Vocab-parallel embeddings. The local lookup is combined
          and immediately sharded into the sequence-parallel layout.
        </p>
        <div class="grid-two">
          <div>
            <h3>Step-by-step</h3>
            <ul>
              <li>Start from local token IDs (your shard of the batch).</li>
              <li>Mask tokens not in the local vocab shard.</li>
              <li>Lookup embeddings in the local vocab slice.</li>
              <li>ReduceScatter across vp to sum partial embeddings.</li>
              <li>Scatter across sp to form the sequence shard.</li>
            </ul>
          </div>
          <div>
            <h3>Shapes</h3>
            <ul>
              <li>Input: [B/dp, S/cp]</li>
              <li>Local lookup: [B/dp, S/cp, D] (unreduced)</li>
              <li>Output: [B/dp, S/(cp*sp), D]</li>
            </ul>
          </div>
        </div>
        <div class="grid-two">
          <div>
            <h3>Compute</h3>
            <ul>
              <li>Each GPU holds a vocab shard of the embedding table.</li>
              <li>Local lookup returns embeddings for in-shard tokens.</li>
              <li>Out-of-shard tokens contribute zeros.</li>
            </ul>
          </div>
          <div>
            <h3>Collectives</h3>
            <ul>
              <li>
                ReduceScatter sums partial embeddings and immediately shards
                the sequence across SP, avoiding a full AllReduce.
              </li>
            </ul>
          </div>
        </div>
      </section>

      <section class="card" id="attention">
        <h2>Attention (SP + CP + TP)</h2>
        <div class="diagram">
          <svg viewBox="0 0 600 850" role="img" aria-label="Attention flow">
            <defs>
              <marker id="arrow-attn" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                <path d="M0,0 L10,3 L0,6 Z" class="arrow-head" />
              </marker>
            </defs>

            <!-- Input -->
            <rect x="225" y="20" width="150" height="40" class="box-meta" />
            <text x="300" y="40" class="text-label">Local Input</text>
            <line x1="300" y1="60" x2="300" y2="90" class="arrow-line" marker-end="url(#arrow-attn)" />
            <text x="370" y="75" class="text-shape">B/dp, S/(cp*sp), D</text>

            <!-- Norm -->
            <rect x="225" y="90" width="150" height="40" class="box-compute" />
            <text x="300" y="110" class="text-label">Norm</text>
            <line x1="300" y1="130" x2="300" y2="160" class="arrow-line" marker-end="url(#arrow-attn)" />

            <!-- AllGather -->
            <rect x="150" y="160" width="300" height="40" class="box-collective" />
            <text x="300" y="180" class="text-label">AllGather(gather_over='sp')</text>
            <line x1="300" y1="200" x2="300" y2="230" class="arrow-line" marker-end="url(#arrow-attn)" />
            <text x="370" y="215" class="text-shape">B/dp, S/cp, D</text>

            <!-- Split lines -->
            <line x1="300" y1="230" x2="100" y2="250" class="arrow-line" marker-end="url(#arrow-attn)" />
            <line x1="300" y1="230" x2="300" y2="250" class="arrow-line" marker-end="url(#arrow-attn)" />
            <line x1="300" y1="230" x2="500" y2="250" class="arrow-line" marker-end="url(#arrow-attn)" />

            <!-- Projections -->
            <rect x="20" y="250" width="160" height="60" class="box-compute" />
            <text x="100" y="270" class="text-label">Q Proj</text>
            <text x="100" y="290" class="text-sub">Wq: D, N/tp, H</text>

            <rect x="220" y="250" width="160" height="60" class="box-compute" />
            <text x="300" y="270" class="text-label">K Proj</text>
            <text x="300" y="290" class="text-sub">Wk: D, G/tp, H</text>

            <rect x="420" y="250" width="160" height="60" class="box-compute" />
            <text x="500" y="270" class="text-label">V Proj</text>
            <text x="500" y="290" class="text-sub">Wv: D, G/tp, H</text>

            <!-- Converge to SDPA -->
            <line x1="100" y1="310" x2="150" y2="350" class="arrow-line" marker-end="url(#arrow-attn)" />
            <line x1="300" y1="310" x2="300" y2="350" class="arrow-line" marker-end="url(#arrow-attn)" />
            <line x1="500" y1="310" x2="450" y2="350" class="arrow-line" marker-end="url(#arrow-attn)" />

            <!-- Context Parallel SDPA -->
            <rect x="100" y="350" width="400" height="60" class="box-compute" />
            <text x="300" y="370" class="text-label">Context Parallel SDPA</text>
            <text x="300" y="390" class="text-sub">(Gather KV over 'cp' in ring)</text>

            <line x1="300" y1="410" x2="300" y2="450" class="arrow-line" marker-end="url(#arrow-attn)" />
            <text x="380" y="430" class="text-shape">B/dp, S/cp, N/tp, H</text>

            <!-- Output Proj -->
            <rect x="200" y="450" width="200" height="60" class="box-compute" />
            <text x="300" y="470" class="text-label">O Proj</text>
            <text x="300" y="490" class="text-sub">Wo: N/tp, H, D</text>

            <line x1="300" y1="510" x2="300" y2="550" class="arrow-line" marker-end="url(#arrow-attn)" />
            <text x="400" y="530" class="text-shape">B/dp, S/cp, D [unreduced]</text>

            <!-- ReduceScatter -->
            <rect x="150" y="550" width="300" height="80" class="box-collective" />
            <text x="300" y="575" class="text-label">ReduceScatter</text>
            <text x="300" y="595" class="text-sub">sum, reduce_over='tp'</text>
            <text x="300" y="615" class="text-sub">scatter_over='sp'</text>

            <line x1="300" y1="630" x2="300" y2="670" class="arrow-line" marker-end="url(#arrow-attn)" />
            <text x="380" y="650" class="text-shape">B/dp, S/(cp*sp), D</text>

            <!-- Residual -->
             <rect x="225" y="670" width="150" height="40" class="box-compute" />
            <text x="300" y="690" class="text-label">Residual Add</text>

            <!-- Exit -->
             <line x1="300" y1="710" x2="300" y2="740" class="arrow-line" marker-end="url(#arrow-attn)" />
          </svg>
        </div>
        <p class="diagram-caption">
          Diagram 2: Attention restores sequence context, circulates K/V over
          the context-parallel ring, then returns to sequence shards.
        </p>
        <div class="grid-two">
          <div>
            <h3>Step-by-step</h3>
            <ul>
              <li>LayerNorm on local shard.</li>
              <li>AllGather over sp to rebuild sequence for attention.</li>
              <li>Project Q, K, V (tp shards heads or head dim).</li>
              <li>CP ring exchanges K/V for full context.</li>
              <li>SDPA computes attention output.</li>
              <li>Output projection (row-parallel) produces partial sums.</li>
              <li>ReduceScatter sums tp and scatters over sp.</li>
            </ul>
          </div>
          <div>
            <h3>Shapes</h3>
            <ul>
              <li>Input: [B/dp, S/(cp*sp), D]</li>
              <li>After AllGather(sp): [B/dp, S/cp, D]</li>
              <li>Output: [B/dp, S/(cp*sp), D]</li>
            </ul>
          </div>
        </div>
        <div class="grid-two">
          <div>
            <h3>Compute</h3>
            <ul>
              <li>Local sequence shards are expanded with AllGather(sp).</li>
              <li>
                Context parallelism circulates K/V so every query can attend
                to all positions.
              </li>
              <li>
                Output projection is often row-parallel in TP, producing partial
                sums.
              </li>
            </ul>
          </div>
          <div>
            <h3>Collectives</h3>
            <ul>
              <li>AllGather(sp) rebuilds the local sequence.</li>
              <li>ReduceScatter returns to the SP-sharded local shape.</li>
            </ul>
          </div>
        </div>
      </section>

      <section class="card" id="mlp">
        <h2>MLP (Dense Feed-Forward)</h2>
        <div class="diagram">
          <svg viewBox="0 0 500 700" role="img" aria-label="MLP flow">
            <defs>
              <marker id="arrow-mlp" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                <path d="M0,0 L10,3 L0,6 Z" class="arrow-head" />
              </marker>
            </defs>

            <!-- Input -->
            <rect x="175" y="20" width="150" height="40" class="box-meta" />
            <text x="250" y="40" class="text-label">Local Input</text>
            <line x1="250" y1="60" x2="250" y2="90" class="arrow-line" marker-end="url(#arrow-mlp)" />
            <text x="320" y="75" class="text-shape">B/dp, S/(cp*sp), D</text>

            <!-- Norm -->
            <rect x="175" y="90" width="150" height="40" class="box-compute" />
            <text x="250" y="110" class="text-label">Norm</text>
            <line x1="250" y1="130" x2="250" y2="160" class="arrow-line" marker-end="url(#arrow-mlp)" />

            <!-- AllGather -->
            <rect x="100" y="160" width="300" height="40" class="box-collective" />
            <text x="250" y="180" class="text-label">AllGather(gather_over='sp')</text>
            <line x1="250" y1="200" x2="250" y2="240" class="arrow-line" marker-end="url(#arrow-mlp)" />
            <text x="320" y="220" class="text-shape">B/dp, S/cp, D</text>

            <!-- Split -->
            <line x1="250" y1="240" x2="150" y2="260" class="arrow-line" marker-end="url(#arrow-mlp)" />
            <line x1="250" y1="240" x2="350" y2="260" class="arrow-line" marker-end="url(#arrow-mlp)" />

            <!-- Projections -->
            <rect x="50" y="260" width="180" height="60" class="box-compute" />
            <text x="140" y="280" class="text-label">Gate Proj</text>
            <text x="140" y="300" class="text-sub">Weight: D, F/tp</text>

            <rect x="270" y="260" width="180" height="60" class="box-compute" />
            <text x="360" y="280" class="text-label">Up Proj</text>
            <text x="360" y="300" class="text-sub">Weight: D, F/tp</text>

            <!-- Activation -->
            <rect x="90" y="340" width="100" height="40" class="box-compute" />
            <text x="140" y="360" class="text-label">GeLU</text>
            <line x1="140" y1="320" x2="140" y2="340" class="arrow-line" marker-end="url(#arrow-mlp)" />
            <line x1="360" y1="320" x2="360" y2="380" class="arrow-line" /> <!-- Byass act for Up -->

            <!-- Elementwise -->
            <rect x="150" y="400" width="200" height="40" class="box-compute" />
            <text x="250" y="420" class="text-label">Element-wise Mult</text>
            <line x1="140" y1="380" x2="190" y2="400" class="arrow-line" marker-end="url(#arrow-mlp)" />
            <line x1="360" y1="380" x2="310" y2="400" class="arrow-line" marker-end="url(#arrow-mlp)" />
            
            <line x1="250" y1="440" x2="250" y2="470" class="arrow-line" marker-end="url(#arrow-mlp)" />
            <text x="320" y="455" class="text-shape">B/dp, S/cp, F/tp</text>

            <!-- Down Proj -->
            <rect x="150" y="470" width="200" height="60" class="box-compute" />
            <text x="250" y="490" class="text-label">Down Proj</text>
            <text x="250" y="510" class="text-sub">Weight: F/tp, D</text>

            <line x1="250" y1="530" x2="250" y2="560" class="arrow-line" marker-end="url(#arrow-mlp)" />
            <text x="350" y="545" class="text-shape">B/dp, S/cp, D [unreduced]</text>

             <!-- ReduceScatter -->
            <rect x="100" y="560" width="300" height="80" class="box-collective" />
            <text x="250" y="585" class="text-label">ReduceScatter</text>
            <text x="250" y="605" class="text-sub">sum, reduce_over='tp'</text>
            <text x="250" y="625" class="text-sub">scatter_over='sp'</text>

            <line x1="250" y1="640" x2="250" y2="670" class="arrow-line" marker-end="url(#arrow-mlp)" />
            <text x="320" y="655" class="text-shape">B/dp, S/(cp*sp), D</text>
          </svg>
        </div>
        <p class="diagram-caption">
          Diagram 3: The MLP uses tensor-parallel projections, then a
          ReduceScatter restores the sequence shard.
        </p>
        <div class="grid-two">
          <div>
            <h3>Step-by-step</h3>
            <ul>
              <li>LayerNorm on local shard.</li>
              <li>AllGather(sp) to rebuild sequence.</li>
              <li>Up projection (tp column parallel) expands to F.</li>
              <li>Activation (GELU or SwiGLU variant).</li>
              <li>Down projection (tp row parallel) back to D.</li>
              <li>ReduceScatter sums tp and scatters over sp.</li>
            </ul>
          </div>
          <div>
            <h3>Shapes</h3>
            <ul>
              <li>Input: [B/dp, S/(cp*sp), D]</li>
              <li>Expanded: [B/dp, S/cp, F/tp]</li>
              <li>Output: [B/dp, S/(cp*sp), D]</li>
            </ul>
          </div>
        </div>
        <div class="grid-two">
          <div>
            <h3>Compute</h3>
            <ul>
              <li>The sequence is gathered, then expanded in the up projection.</li>
              <li>Activation happens locally on each shard.</li>
              <li>The down projection produces partial sums.</li>
            </ul>
          </div>
          <div>
            <h3>Collectives</h3>
            <ul>
              <li>AllGather(sp) before the MLP to rebuild sequence.</li>
              <li>
                ReduceScatter after the row-parallel projection to return to the
                SP shard.
              </li>
            </ul>
          </div>
        </div>
      </section>

      <section class="card" id="moe">
        <h2>Mixture of Experts (MoE)</h2>
        <div class="diagram">
          <svg viewBox="0 0 500 500" role="img" aria-label="MoE flow">
            <defs>
              <marker id="arrow-moe" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                <path d="M0,0 L10,3 L0,6 Z" class="arrow-head" />
              </marker>
            </defs>

            <!-- Input -->
            <rect x="150" y="20" width="200" height="40" class="box-meta" />
            <text x="250" y="40" class="text-label">Local Input</text>
            <line x1="250" y1="60" x2="250" y2="90" class="arrow-line" marker-end="url(#arrow-moe)" />
            <text x="320" y="75" class="text-shape">B/dp, S/(cp*sp), D</text>

            <!-- Router -->
            <rect x="100" y="90" width="300" height="60" class="box-compute" />
            <text x="250" y="110" class="text-label">Router (Top-K)</text>
            <text x="250" y="130" class="text-sub">Select experts per token</text>

            <line x1="250" y1="150" x2="250" y2="180" class="arrow-line" marker-end="url(#arrow-moe)" />
            
            <!-- AllToAll Dispatch -->
            <rect x="100" y="180" width="300" height="60" class="box-collective" />
            <text x="250" y="200" class="text-label">AllToAll (Dispatch)</text>
            <text x="250" y="220" class="text-sub">Send tokens to expert owners (EP)</text>
            
            <line x1="250" y1="240" x2="250" y2="270" class="arrow-line" marker-end="url(#arrow-moe)" />
            <text x="320" y="255" class="text-shape">Tokens per Expert</text>

            <!-- Expert MLP -->
            <rect x="100" y="270" width="300" height="60" class="box-compute" />
            <text x="250" y="290" class="text-label">Expert MLP Block</text>
            <text x="250" y="310" class="text-sub">Process assigned tokens (opt. TP)</text>

            <line x1="250" y1="330" x2="250" y2="360" class="arrow-line" marker-end="url(#arrow-moe)" />

            <!-- AllToAll Combine -->
            <rect x="100" y="360" width="300" height="60" class="box-collective" />
            <text x="250" y="380" class="text-label">AllToAll (Combine)</text>
            <text x="250" y="400" class="text-sub">Return results to source rank</text>

            <line x1="250" y1="420" x2="250" y2="450" class="arrow-line" marker-end="url(#arrow-moe)" />
            <text x="320" y="435" class="text-shape">B/dp, S/(cp*sp), D</text>
          </svg>
        </div>
        <p class="diagram-caption">
          Diagram 4: Routing and AllToAll communication dominates MoE. The
          expert MLP can still use tensor parallelism internally.
        </p>
        <div class="grid-two">
          <div>
            <h3>Step-by-step</h3>
            <ul>
              <li>Router selects top‑k experts per token.</li>
              <li>AllToAll dispatches tokens to expert owners (ep).</li>
              <li>Experts run local MLP (optionally tp).</li>
              <li>AllToAll returns expert outputs.</li>
              <li>Combine outputs with routing weights.</li>
            </ul>
          </div>
          <div>
            <h3>Shapes</h3>
            <ul>
              <li>Input: [B/dp, S/(cp*sp), D]</li>
              <li>After routing: token‑expert batches</li>
              <li>Output: [B/dp, S/(cp*sp), D]</li>
            </ul>
          </div>
        </div>
        <div class="grid-two">
          <div>
            <h3>Compute</h3>
            <ul>
              <li>Tokens are routed to experts based on a gating network.</li>
              <li>Each expert runs an MLP on its assigned tokens.</li>
              <li>Expert compute can itself be tensor-parallel.</li>
            </ul>
          </div>
          <div>
            <h3>Collectives</h3>
            <ul>
              <li>AllToAll dispatches tokens to their experts.</li>
              <li>AllToAll returns results to original positions.</li>
            </ul>
          </div>
        </div>
      </section>

      <section class="card" id="loss">
        <h2>Loss (Vocab Parallel Cross Entropy)</h2>
        <div class="diagram">
          <svg viewBox="0 0 500 450" role="img" aria-label="Loss flow">
            <defs>
              <marker id="arrow-loss" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                <path d="M0,0 L10,3 L0,6 Z" class="arrow-head" />
              </marker>
            </defs>

            <!-- Input -->
             <rect x="150" y="20" width="200" height="40" class="box-meta" />
            <text x="250" y="40" class="text-label">Local Logits</text>
            <line x1="250" y1="60" x2="250" y2="90" class="arrow-line" marker-end="url(#arrow-loss)" />
            <text x="320" y="75" class="text-shape">V/vp</text>

            <!-- AllReduce Max -->
            <rect x="100" y="90" width="300" height="50" class="box-collective" />
            <text x="250" y="115" class="text-label">AllReduce(max)</text>
            <line x1="250" y1="140" x2="250" y2="170" class="arrow-line" marker-end="url(#arrow-loss)" />

            <!-- AllReduce Sum -->
            <rect x="100" y="170" width="300" height="50" class="box-collective" />
            <text x="250" y="195" class="text-label">AllReduce(sum)</text>
            <line x1="250" y1="220" x2="250" y2="250" class="arrow-line" marker-end="url(#arrow-loss)" />

            <!-- Target Masking -->
            <rect x="100" y="250" width="300" height="60" class="box-compute" />
            <text x="250" y="270" class="text-label">Mask Targets</text>
            <text x="250" y="290" class="text-sub">Identify local target logit</text>
            
            <line x1="250" y1="310" x2="250" y2="340" class="arrow-line" marker-end="url(#arrow-loss)" />

            <!-- AllReduce Target -->
            <rect x="100" y="340" width="300" height="50" class="box-collective" />
            <text x="250" y="365" class="text-label">AllReduce(sum)</text>
            <text x="250" y="385" class="text-sub">Broadcast target logit</text>

             <line x1="250" y1="390" x2="250" y2="420" class="arrow-line" marker-end="url(#arrow-loss)" />
             <text x="320" y="405" class="text-shape">Scalar Loss</text>
          </svg>
        </div>
        <p class="diagram-caption">
          Diagram 5: Vocab-parallel softmax uses global max and sum; target
          logits are masked and reduced across shards.
        </p>
        <div class="grid-two">
          <div>
            <h3>Step-by-step</h3>
            <ul>
              <li>Compute local logits for vocab shard.</li>
              <li>AllReduce max for numerical stability.</li>
              <li>AllReduce sum for softmax denominator.</li>
              <li>Mask target logit if not local.</li>
              <li>AllReduce sum to share target logit.</li>
            </ul>
          </div>
          <div>
            <h3>Shapes</h3>
            <ul>
              <li>Logits: [B/dp, S/(cp*sp), V/vp]</li>
              <li>Targets: [B/dp, S/(cp*sp)]</li>
              <li>Loss: scalar or per-token</li>
            </ul>
          </div>
        </div>
        <div class="grid-two">
          <div>
            <h3>Compute</h3>
            <ul>
              <li>Each GPU computes logits for its vocab slice.</li>
              <li>Softmax uses global max and sum for stability.</li>
              <li>Only the GPU owning the target class has the true logit.</li>
            </ul>
          </div>
          <div>
            <h3>Collectives</h3>
            <ul>
              <li>AllReduce for max and sum across vocab shards.</li>
              <li>AllReduce to share the correct target logit.</li>
            </ul>
          </div>
        </div>
      </section>

      <section class="card note" id="fsdp">
        <h2>Note on FSDP</h2>
        <p>
          Fully Sharded Data Parallel (FSDP) is a memory optimization that
          shards weights and optimizer states at rest, then gathers weights
          just-in-time for forward/backward. It does not change the local tensor
          shapes that participate in the compute steps above, which is why it is
          often discussed separately from compute-parallel strategies.
        </p>
      </section>
      </div>
    </main>

    <footer class="footer">
      <div class="container">
        <p>
          Built for GitHub Pages. Replace diagrams, adjust labels, and extend
          sections as needed.
        </p>
      </div>
    </footer>
  </body>
</html>
