<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Visualizing Parallelism in Transformer Compute</title>
    <meta
      name="description"
      content="A step-by-step, diagram-driven explanation of distributed compute in transformer training: DP, TP, SP, CP, EP, VP."
    />
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <header class="hero">
      <div class="container">
        <p class="eyebrow">Distributed Training Notes</p>
        <h1>Visualizing Parallelism in Transformer Compute</h1>
        <p class="subtitle">
          This page is a map, not a manual. It keeps the tensor shapes visible,
          names the collectives at each step, and shows how each GPU’s local
          work is stitched into the global result. The emphasis is on clarity:
          where the sequence is gathered, where weights are sharded, and how the
          model returns to its memory‑efficient local form after each block.
        </p>
        <div class="hero-meta">
          <span>Author: Harshitha M</span>
          <span>Updated: Feb 2, 2026</span>
        </div>
      </div>
    </header>

    <nav class="top-nav">
      <div class="container">
        <a href="#reading-guide">Reading guide</a>
        <a href="#symbols">Symbols</a>
        <a href="#embeddings">Embeddings</a>
        <a href="#attention">Attention</a>
        <a href="#mlp">MLP</a>
        <a href="#moe">MoE</a>
        <a href="#loss">Loss</a>
      </div>
    </nav>

    <main class="container layout">
      <aside class="toc">
        <div class="toc-card">
          <h2>Contents</h2>
          <a href="#reading-guide">How to read</a>
          <a href="#symbols">Symbols</a>
          <a href="#local-shape">Local shape</a>
          <a href="#overview">Overview</a>
          <a href="#embeddings">Embeddings</a>
          <a href="#attention">Attention</a>
          <a href="#mlp">MLP</a>
          <a href="#moe">MoE</a>
          <a href="#loss">Loss</a>
          <a href="#fsdp">FSDP note</a>
        </div>
      </aside>

      <div class="content">
      <section class="card" id="reading-guide">
        <h2>How to Read the Diagrams</h2>
        <p>
          Think from the point of view of a <strong>single GPU</strong>. Each
          diagram answers two questions: (1) what local tensor shape does this
          GPU hold, and (2) which collective operations are needed to connect
          local compute into a correct global result.
        </p>
        <p>
          When you see the word <em>local</em>, it means the tensor fragment on
          one device. When you see a collective (AllGather, ReduceScatter,
          AllReduce, AllToAll), it means the GPU is coordinating with its
          process group to assemble or redistribute a global tensor.
        </p>
        <div class="reading-list">
          <div>
            <h3>Reading cues</h3>
            <ul>
              <li>Rectangles: local compute blocks.</li>
              <li>Arrows: data movement between blocks.</li>
              <li>Collectives: synchronization boundaries.</li>
            </ul>
          </div>
          <div>
            <h3>Goal</h3>
            <ul>
              <li>Track how a local shard becomes global.</li>
              <li>Track how global work returns to a shard.</li>
            </ul>
          </div>
        </div>
      </section>

      <section class="grid-two" id="symbols">
        <div class="card">
          <h2>Model Shape Symbols</h2>
          <table>
            <thead>
              <tr>
                <th>Symbol</th>
                <th>Meaning</th>
                <th>Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>B</td>
                <td>Batch size</td>
                <td>Global batch count (number of sequences)</td>
              </tr>
              <tr>
                <td>S</td>
                <td>Sequence length</td>
                <td>Context window size</td>
              </tr>
              <tr>
                <td>D</td>
                <td>Hidden size</td>
                <td>Width of the residual stream</td>
              </tr>
              <tr>
                <td>V</td>
                <td>Vocabulary size</td>
                <td>Total token count in the embedding table</td>
              </tr>
              <tr>
                <td>F</td>
                <td>FFN size</td>
                <td>Expansion dimension in the MLP</td>
              </tr>
              <tr>
                <td>E</td>
                <td>Number of experts</td>
                <td>Total experts in an MoE layer</td>
              </tr>
              <tr>
                <td>C</td>
                <td>Capacity</td>
                <td>Max tokens per expert (MoE specific)</td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="card">
          <h2>Parallel Configuration</h2>
          <table>
            <thead>
              <tr>
                <th>Symbol</th>
                <th>Parallelism</th>
                <th>Shard Target</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>dp</td>
                <td>Data parallel</td>
                <td>Batch dimension (B)</td>
              </tr>
              <tr>
                <td>tp</td>
                <td>Tensor parallel</td>
                <td>Weights (heads in attention, F in MLP)</td>
              </tr>
              <tr>
                <td>sp</td>
                <td>Sequence parallel</td>
                <td>Sequence activations (S)</td>
              </tr>
              <tr>
                <td>cp</td>
                <td>Context parallel</td>
                <td>Sequence dimension in attention (QKV)</td>
              </tr>
              <tr>
                <td>ep</td>
                <td>Expert parallel</td>
                <td>Experts (E) in MoE layers</td>
              </tr>
              <tr>
                <td>vp</td>
                <td>Vocab parallel</td>
                <td>Vocabulary table (V)</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>

      <section class="card highlight" id="local-shape">
        <h2>The Local Shape You Will See Repeatedly</h2>
        <p>
          A typical block enters and exits with the same local activation shape:
        </p>
        <p class="code-block">
          <code>[B/dp, S/(cp*sp), D]</code>
        </p>
        <p>
          Read this as: this GPU holds a fraction of the batch, a small slice of
          the sequence (sharded by context and sequence parallelism), and the
          full hidden dimension.
        </p>
      </section>

      <section class="card" id="overview">
        <h2>Overview of the Transformer Layer</h2>
        <p>
          A standard layer consists of embeddings (global input lookup), an
          attention block, and an MLP (dense or MoE). Each block starts from a
          local sequence shard, temporarily gathers what it needs, computes, and
          then returns to the memory-efficient local shape via collective
          operations.
        </p>
        <p class="section-lead">
          The rest of the page walks through the same rhythm repeatedly:
          gather what you need, compute locally, then reduce or scatter back to
          the local shape.
        </p>
      </section>

      <section class="card" id="embeddings">
        <h2>Embeddings (Vocab Parallel)</h2>
        <div class="diagram">
          <svg viewBox="0 0 900 160" role="img" aria-label="Embedding flow">
            <defs>
              <marker
                id="arrow"
                markerWidth="10"
                markerHeight="10"
                refX="9"
                refY="3"
                orient="auto"
              >
                <path d="M0,0 L10,3 L0,6 Z" fill="#1c2b4a" />
              </marker>
            </defs>
            <rect x="20" y="30" width="180" height="60" rx="10" />
            <text x="110" y="66">Token IDs</text>

            <line
              x1="200"
              y1="60"
              x2="300"
              y2="60"
              marker-end="url(#arrow)"
            />

            <rect x="300" y="30" width="220" height="60" rx="10" />
            <text x="410" y="58">VP Embedding</text>
            <text x="410" y="78">Local slice</text>

            <line
              x1="520"
              y1="60"
              x2="640"
              y2="60"
              marker-end="url(#arrow)"
            />

            <rect x="640" y="30" width="220" height="60" rx="10" />
            <text x="750" y="58">ReduceScatter</text>
            <text x="750" y="78">Sum + SP shard</text>
          </svg>
        </div>
        <p class="diagram-caption">
          Diagram 1: Vocab-parallel embeddings. The local lookup is combined
          and immediately sharded into the sequence-parallel layout.
        </p>
        <div class="grid-two">
          <div>
            <h3>Compute</h3>
            <ul>
              <li>Each GPU holds a vocab shard of the embedding table.</li>
              <li>Local lookup returns embeddings for in-shard tokens.</li>
              <li>Out-of-shard tokens contribute zeros.</li>
            </ul>
          </div>
          <div>
            <h3>Collectives</h3>
            <ul>
              <li>
                ReduceScatter sums partial embeddings and immediately shards
                the sequence across SP, avoiding a full AllReduce.
              </li>
            </ul>
          </div>
        </div>
      </section>

      <section class="card" id="attention">
        <h2>Attention (SP + CP + TP)</h2>
        <div class="diagram">
          <svg viewBox="0 0 900 180" role="img" aria-label="Attention flow">
            <defs>
              <marker
                id="arrow-attn"
                markerWidth="10"
                markerHeight="10"
                refX="9"
                refY="3"
                orient="auto"
              >
                <path d="M0,0 L10,3 L0,6 Z" fill="#1c2b4a" />
              </marker>
            </defs>
            <rect x="20" y="60" width="160" height="60" rx="10" />
            <text x="100" y="96">Local shape</text>

            <line
              x1="180"
              y1="90"
              x2="280"
              y2="90"
              marker-end="url(#arrow-attn)"
            />

            <rect x="280" y="40" width="160" height="100" rx="10" />
            <text x="360" y="78">AllGather</text>
            <text x="360" y="98">SP sequence</text>

            <line
              x1="440"
              y1="90"
              x2="560"
              y2="90"
              marker-end="url(#arrow-attn)"
            />

            <rect x="560" y="40" width="170" height="100" rx="10" />
            <text x="645" y="70">Attention</text>
            <text x="645" y="90">CP ring for K/V</text>
            <text x="645" y="110">Q stays local</text>

            <line
              x1="730"
              y1="90"
              x2="760"
              y2="90"
              marker-end="url(#arrow-attn)"
            />

            <rect x="760" y="50" width="120" height="80" rx="10" />
            <text x="820" y="82">ReduceScatter</text>
          </svg>
        </div>
        <p class="diagram-caption">
          Diagram 2: Attention restores sequence context, circulates K/V over
          the context-parallel ring, then returns to sequence shards.
        </p>
        <div class="grid-two">
          <div>
            <h3>Compute</h3>
            <ul>
              <li>Local sequence shards are expanded with AllGather(sp).</li>
              <li>
                Context parallelism circulates K/V so every query can attend
                to all positions.
              </li>
              <li>
                Output projection is often row-parallel in TP, producing partial
                sums.
              </li>
            </ul>
          </div>
          <div>
            <h3>Collectives</h3>
            <ul>
              <li>AllGather(sp) rebuilds the local sequence.</li>
              <li>ReduceScatter returns to the SP-sharded local shape.</li>
            </ul>
          </div>
        </div>
      </section>

      <section class="card" id="mlp">
        <h2>MLP (Dense Feed-Forward)</h2>
        <div class="diagram">
          <svg viewBox="0 0 900 160" role="img" aria-label="MLP flow">
            <defs>
              <marker
                id="arrow-mlp"
                markerWidth="10"
                markerHeight="10"
                refX="9"
                refY="3"
                orient="auto"
              >
                <path d="M0,0 L10,3 L0,6 Z" fill="#1c2b4a" />
              </marker>
            </defs>
            <rect x="20" y="50" width="150" height="60" rx="10" />
            <text x="95" y="86">Local shape</text>

            <line x1="170" y1="80" x2="260" y2="80" marker-end="url(#arrow-mlp)" />
            <rect x="260" y="40" width="160" height="80" rx="10" />
            <text x="340" y="70">AllGather</text>
            <text x="340" y="90">SP sequence</text>

            <line x1="420" y1="80" x2="520" y2="80" marker-end="url(#arrow-mlp)" />
            <rect x="520" y="40" width="150" height="80" rx="10" />
            <text x="595" y="70">Up proj</text>
            <text x="595" y="90">TP column</text>

            <line x1="670" y1="80" x2="770" y2="80" marker-end="url(#arrow-mlp)" />
            <rect x="770" y="40" width="110" height="80" rx="10" />
            <text x="825" y="70">Down proj</text>
            <text x="825" y="90">TP row</text>
          </svg>
        </div>
        <p class="diagram-caption">
          Diagram 3: The MLP uses tensor-parallel projections, then a
          ReduceScatter restores the sequence shard.
        </p>
        <div class="grid-two">
          <div>
            <h3>Compute</h3>
            <ul>
              <li>The sequence is gathered, then expanded in the up projection.</li>
              <li>Activation happens locally on each shard.</li>
              <li>The down projection produces partial sums.</li>
            </ul>
          </div>
          <div>
            <h3>Collectives</h3>
            <ul>
              <li>AllGather(sp) before the MLP to rebuild sequence.</li>
              <li>
                ReduceScatter after the row-parallel projection to return to the
                SP shard.
              </li>
            </ul>
          </div>
        </div>
      </section>

      <section class="card" id="moe">
        <h2>Mixture of Experts (MoE)</h2>
        <div class="diagram">
          <svg viewBox="0 0 900 180" role="img" aria-label="MoE flow">
            <defs>
              <marker
                id="arrow-moe"
                markerWidth="10"
                markerHeight="10"
                refX="9"
                refY="3"
                orient="auto"
              >
                <path d="M0,0 L10,3 L0,6 Z" fill="#1c2b4a" />
              </marker>
            </defs>
            <rect x="20" y="60" width="150" height="60" rx="10" />
            <text x="95" y="96">Router (top-k)</text>

            <line x1="170" y1="90" x2="280" y2="90" marker-end="url(#arrow-moe)" />
            <rect x="280" y="40" width="170" height="100" rx="10" />
            <text x="365" y="70">AllToAll</text>
            <text x="365" y="90">Dispatch to EP</text>

            <line x1="450" y1="90" x2="590" y2="90" marker-end="url(#arrow-moe)" />
            <rect x="590" y="40" width="180" height="100" rx="10" />
            <text x="680" y="70">Expert MLP</text>
            <text x="680" y="90">Optional TP</text>

            <line x1="770" y1="90" x2="780" y2="90" marker-end="url(#arrow-moe)" />
            <rect x="780" y="50" width="110" height="80" rx="10" />
            <text x="835" y="82">AllToAll</text>
          </svg>
        </div>
        <p class="diagram-caption">
          Diagram 4: Routing and AllToAll communication dominates MoE. The
          expert MLP can still use tensor parallelism internally.
        </p>
        <div class="grid-two">
          <div>
            <h3>Compute</h3>
            <ul>
              <li>Tokens are routed to experts based on a gating network.</li>
              <li>Each expert runs an MLP on its assigned tokens.</li>
              <li>Expert compute can itself be tensor-parallel.</li>
            </ul>
          </div>
          <div>
            <h3>Collectives</h3>
            <ul>
              <li>AllToAll dispatches tokens to their experts.</li>
              <li>AllToAll returns results to original positions.</li>
            </ul>
          </div>
        </div>
      </section>

      <section class="card" id="loss">
        <h2>Loss (Vocab Parallel Cross Entropy)</h2>
        <div class="diagram">
          <svg viewBox="0 0 900 180" role="img" aria-label="Loss flow">
            <defs>
              <marker
                id="arrow-loss"
                markerWidth="10"
                markerHeight="10"
                refX="9"
                refY="3"
                orient="auto"
              >
                <path d="M0,0 L10,3 L0,6 Z" fill="#1c2b4a" />
              </marker>
            </defs>
            <rect x="20" y="60" width="170" height="60" rx="10" />
            <text x="105" y="96">Logits (VP shard)</text>

            <line x1="190" y1="90" x2="310" y2="90" marker-end="url(#arrow-loss)" />
            <rect x="310" y="40" width="170" height="100" rx="10" />
            <text x="395" y="70">AllReduce</text>
            <text x="395" y="90">Global max</text>
            <text x="395" y="110">Global sum</text>

            <line x1="480" y1="90" x2="610" y2="90" marker-end="url(#arrow-loss)" />
            <rect x="610" y="40" width="170" height="100" rx="10" />
            <text x="695" y="70">Mask targets</text>
            <text x="695" y="90">Local label if in shard</text>
            <text x="695" y="110">AllReduce sum</text>
          </svg>
        </div>
        <p class="diagram-caption">
          Diagram 5: Vocab-parallel softmax uses global max and sum; target
          logits are masked and reduced across shards.
        </p>
        <div class="grid-two">
          <div>
            <h3>Compute</h3>
            <ul>
              <li>Each GPU computes logits for its vocab slice.</li>
              <li>Softmax uses global max and sum for stability.</li>
              <li>Only the GPU owning the target class has the true logit.</li>
            </ul>
          </div>
          <div>
            <h3>Collectives</h3>
            <ul>
              <li>AllReduce for max and sum across vocab shards.</li>
              <li>AllReduce to share the correct target logit.</li>
            </ul>
          </div>
        </div>
      </section>

      <section class="card note" id="fsdp">
        <h2>Note on FSDP</h2>
        <p>
          Fully Sharded Data Parallel (FSDP) is a memory optimization that
          shards weights and optimizer states at rest, then gathers weights
          just-in-time for forward/backward. It does not change the local tensor
          shapes that participate in the compute steps above, which is why it is
          often discussed separately from compute-parallel strategies.
        </p>
      </section>
      </div>
    </main>

    <footer class="footer">
      <div class="container">
        <p>
          Built for GitHub Pages. Replace diagrams, adjust labels, and extend
          sections as needed.
        </p>
      </div>
    </footer>
  </body>
</html>
